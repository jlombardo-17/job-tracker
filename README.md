# Job Tracker

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org/)
[![GitHub Repo](https://img.shields.io/badge/github-job--tracker-blue?logo=github)](https://github.com/jlombardo-17/job-tracker)

Sistema automatizado de seguimiento de ofertas de trabajo y llamados desde mÃºltiples fuentes web uruguayas.

![Job Tracker](https://img.shields.io/badge/Status-Active-success)

## ğŸš€ CaracterÃ­sticas

- **Scraping automÃ¡tico** de ofertas de trabajo desde mÃºltiples sitios
- **Base de datos SQLite** para almacenamiento eficiente
- **API REST** completa para gestiÃ³n de datos
- **Frontend moderno** con filtros y bÃºsqueda en tiempo real
- **Sistema de logs** para seguimiento de scraping
- **ConfiguraciÃ³n flexible** de fuentes de datos

## ğŸ“‹ Requisitos

- Node.js >= 18.0.0
- npm o yarn

## ğŸ”§ InstalaciÃ³n

1. Clonar el repositorio
2. Instalar dependencias:

```bash
npm install
```

3. Inicializar la base de datos:

```bash
npm run init-db
```

## ğŸ¯ Uso

### Iniciar el servidor

```bash
npm start
```

El servidor estarÃ¡ disponible en `http://localhost:3000`

### Modo desarrollo (con auto-reload)

```bash
npm run dev
```

### Ejecutar scraping manual

```bash
node scripts/run-scraper.js
```

## ğŸ“ Estructura del Proyecto

```
job-tracker/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes/          # Rutas de la API REST
â”œâ”€â”€ config/              # ConfiguraciÃ³n de BD y fuentes
â”œâ”€â”€ data/                # Base de datos SQLite
â”œâ”€â”€ logs/                # Logs del sistema
â”œâ”€â”€ public/              # Frontend estÃ¡tico
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ js/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ scripts/             # Scripts de utilidad
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # Modelos de datos
â”‚   â”œâ”€â”€ services/        # Servicios de scraping
â”‚   â””â”€â”€ utilities/       # Utilidades y scheduler
â””â”€â”€ server.js            # Servidor Express
```

## ğŸ”Œ API Endpoints

### Jobs

- `GET /api/jobs` - Obtener todos los trabajos (con filtros)
- `GET /api/jobs/:id` - Obtener trabajo por ID
- `GET /api/jobs/stats/overview` - EstadÃ­sticas generales
- `PATCH /api/jobs/:id/deactivate` - Marcar trabajo como inactivo

### Sources

- `GET /api/sources` - Obtener todas las fuentes
- `GET /api/sources/:id` - Obtener fuente por ID
- `GET /api/sources/:id/stats` - EstadÃ­sticas de una fuente
- `PATCH /api/sources/:id/toggle` - Activar/desactivar fuente

### Scraper

- `POST /api/scraper/all` - Ejecutar scraping de todas las fuentes
- `POST /api/scraper/source/:sourceId` - Ejecutar scraping de una fuente
- `GET /api/scraper/logs` - Obtener logs de scraping
- `GET /api/scraper/logs/:sourceId` - Obtener logs de una fuente

## âš™ï¸ ConfiguraciÃ³n

### Fuentes de datos

Editar `config/sources.json` para agregar o modificar fuentes:

```json
{
  "sources": [
    {
      "id": "mi-fuente",
      "name": "Mi Fuente de Trabajos",
      "url": "https://ejemplo.com",
      "enabled": true,
      "category": "general",
      "scraper": "mi-scraper"
    }
  ]
}
```

### Variables de entorno

Crear archivo `.env` en la raÃ­z:

```env
PORT=3000
NODE_ENV=development
DB_PATH=./data/jobs.db
LOG_LEVEL=info
```

## ğŸ•·ï¸ Agregar nuevos scrapers

1. Editar `src/services/ScraperService.js`
2. Agregar un nuevo mÃ©todo de scraping:

```javascript
async scrapeNuevoSitio() {
  const jobs = [];
  const response = await axios.get('https://sitio.com', this.axiosConfig);
  const $ = cheerio.load(response.data);
  
  // Adaptar selectores al sitio especÃ­fico
  $('.job-item').each((i, elem) => {
    jobs.push({
      external_id: $(elem).attr('data-id'),
      title: $(elem).find('.title').text().trim(),
      company: $(elem).find('.company').text().trim(),
      // ... mÃ¡s campos
    });
  });
  
  return jobs;
}
```

3. Agregar el caso en el switch de `scrapeSource()`

## ğŸ“Š Base de Datos

El sistema utiliza SQLite con las siguientes tablas:

- **jobs** - Ofertas de trabajo
- **sources** - Fuentes de datos
- **scraping_log** - Logs de ejecuciÃ³n

## ğŸ¨ Frontend

El frontend es una Single Page Application (SPA) con:

- DiseÃ±o responsive
- Filtros en tiempo real
- Modal de detalles
- Notificaciones toast
- Tema oscuro moderno

## ğŸ”„ Scraping AutomÃ¡tico

Para habilitar scraping automÃ¡tico periÃ³dico, agregar al `server.js`:

```javascript
import Scheduler from './src/utilities/Scheduler.js';

// Ejecutar cada 6 horas
const scheduler = new Scheduler(6);
scheduler.start();
```

## ğŸ“ Notas

- Los scrapers incluidos son ejemplos bÃ¡sicos que necesitan adaptarse a la estructura real de cada sitio
- Se recomienda revisar los tÃ©rminos de servicio de cada sitio antes de hacer scraping
- Para entornos de producciÃ³n, considerar usar proxies y rate limiting

## ğŸ“„ Licencia

MIT

## ğŸ‘¨â€ğŸ’» Desarrollo

Contribuciones son bienvenidas. Por favor crear un issue antes de enviar PRs.
